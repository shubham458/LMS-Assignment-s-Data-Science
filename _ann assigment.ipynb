{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT THE BURNED AREA OF FOREST FIRES WITH NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('forestfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>aug</td>\n",
       "      <td>sat</td>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>nov</td>\n",
       "      <td>tue</td>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain  ...  monthfeb  \\\n",
       "0     mar  fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0  ...         0   \n",
       "1     oct  tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0  ...         0   \n",
       "2     oct  sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0  ...         0   \n",
       "3     mar  fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2  ...         0   \n",
       "4     mar  sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0  ...         0   \n",
       "..    ...  ...   ...    ...    ...   ...   ...  ..   ...   ...  ...       ...   \n",
       "512   aug  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0  ...         0   \n",
       "513   aug  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  ...         0   \n",
       "514   aug  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  ...         0   \n",
       "515   aug  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0  ...         0   \n",
       "516   nov  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0  ...         0   \n",
       "\n",
       "     monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
       "0           0         0         0         1         0         0         0   \n",
       "1           0         0         0         0         0         0         1   \n",
       "2           0         0         0         0         0         0         1   \n",
       "3           0         0         0         1         0         0         0   \n",
       "4           0         0         0         1         0         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         1         0   \n",
       "\n",
       "     monthsep  size_category  \n",
       "0           0          small  \n",
       "1           0          small  \n",
       "2           0          small  \n",
       "3           0          small  \n",
       "4           0          small  \n",
       "..        ...            ...  \n",
       "512         0          large  \n",
       "513         0          large  \n",
       "514         0          large  \n",
       "515         0          small  \n",
       "516         0          small  \n",
       "\n",
       "[517 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.drop(['month','day'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['size_category']=lb.fit_transform(df1['size_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 29)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df1.values\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df2[:,0:28]\n",
    "y=df2[:,-1]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=28, activation='relu'))\n",
    "model.add(Dense(28, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 0.3457 - accuracy: 0.8786 - val_loss: 0.4027 - val_accuracy: 0.8421\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9422 - val_loss: 0.1661 - val_accuracy: 0.9357\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.9653 - val_loss: 0.3872 - val_accuracy: 0.8596\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9480 - val_loss: 0.4250 - val_accuracy: 0.8480\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1245 - accuracy: 0.9538 - val_loss: 0.2795 - val_accuracy: 0.8772\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9595 - val_loss: 0.2757 - val_accuracy: 0.8772\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1057 - accuracy: 0.9711 - val_loss: 0.1437 - val_accuracy: 0.9240\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9711 - val_loss: 0.1379 - val_accuracy: 0.9298\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.9624 - val_loss: 0.1980 - val_accuracy: 0.8830\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9653 - val_loss: 0.1683 - val_accuracy: 0.9591\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9682 - val_loss: 0.3444 - val_accuracy: 0.8713\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9653 - val_loss: 0.1532 - val_accuracy: 0.9649\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 0.9711 - val_loss: 0.2272 - val_accuracy: 0.8772\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9422 - val_loss: 0.2584 - val_accuracy: 0.8772\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1228 - accuracy: 0.9538 - val_loss: 0.1889 - val_accuracy: 0.8889\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9827 - val_loss: 0.1401 - val_accuracy: 0.9591\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 0.9653 - val_loss: 0.4205 - val_accuracy: 0.8830\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.9769 - val_loss: 0.1371 - val_accuracy: 0.9298\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9740 - val_loss: 0.2343 - val_accuracy: 0.8830\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9855 - val_loss: 0.1771 - val_accuracy: 0.9064\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.9769 - val_loss: 0.0954 - val_accuracy: 0.9532\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9798 - val_loss: 0.0939 - val_accuracy: 0.9649\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.9653 - val_loss: 0.1400 - val_accuracy: 0.9591\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2772 - accuracy: 0.9277 - val_loss: 0.1282 - val_accuracy: 0.9708\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9595 - val_loss: 0.0827 - val_accuracy: 0.9766\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0452 - accuracy: 0.9913 - val_loss: 0.0912 - val_accuracy: 0.9532\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.9740 - val_loss: 0.1406 - val_accuracy: 0.9532\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9913 - val_loss: 0.0921 - val_accuracy: 0.9649\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 0.9827 - val_loss: 0.1144 - val_accuracy: 0.9649\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9711 - val_loss: 0.1146 - val_accuracy: 0.9532\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9595 - val_loss: 0.4119 - val_accuracy: 0.8772\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9711 - val_loss: 0.7408 - val_accuracy: 0.8596\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9769 - val_loss: 0.1539 - val_accuracy: 0.9298\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.0300 - accuracy: 0.9913 - val_loss: 0.0920 - val_accuracy: 0.9649\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.9827 - val_loss: 0.1537 - val_accuracy: 0.9298\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.9884 - val_loss: 0.1314 - val_accuracy: 0.9649\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.0794 - accuracy: 0.9711 - val_loss: 0.1588 - val_accuracy: 0.9298\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0389 - accuracy: 0.9913 - val_loss: 0.1742 - val_accuracy: 0.9298\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9855 - val_loss: 0.5037 - val_accuracy: 0.8772\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0442 - accuracy: 0.9827 - val_loss: 0.0748 - val_accuracy: 0.9649\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0626 - accuracy: 0.9769 - val_loss: 0.1166 - val_accuracy: 0.9474\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9740 - val_loss: 0.1763 - val_accuracy: 0.9357\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9711 - val_loss: 0.3392 - val_accuracy: 0.8772\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9451 - val_loss: 0.2100 - val_accuracy: 0.9298\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 0.9884 - val_loss: 0.0775 - val_accuracy: 0.9591\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9798 - val_loss: 0.1362 - val_accuracy: 0.9415\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.9798 - val_loss: 0.1938 - val_accuracy: 0.9240\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9827 - val_loss: 0.1843 - val_accuracy: 0.9240\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9682 - val_loss: 0.0957 - val_accuracy: 0.9532\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0435 - accuracy: 0.9884 - val_loss: 0.0767 - val_accuracy: 0.9649\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0348 - accuracy: 0.9827 - val_loss: 0.0798 - val_accuracy: 0.9649\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0872 - accuracy: 0.9682 - val_loss: 0.2033 - val_accuracy: 0.9298\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9913 - val_loss: 0.0875 - val_accuracy: 0.9532\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.9827 - val_loss: 0.1231 - val_accuracy: 0.9415\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9855 - val_loss: 0.0771 - val_accuracy: 0.9591\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9798 - val_loss: 0.0768 - val_accuracy: 0.9649\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.9913 - val_loss: 0.1673 - val_accuracy: 0.9357\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1215 - accuracy: 0.9566 - val_loss: 0.0766 - val_accuracy: 0.9649\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9913 - val_loss: 0.1480 - val_accuracy: 0.9357\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 0.9711 - val_loss: 0.1243 - val_accuracy: 0.9591\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 0.9942 - val_loss: 0.0721 - val_accuracy: 0.9474\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0422 - accuracy: 0.9855 - val_loss: 0.2316 - val_accuracy: 0.9240\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9855 - val_loss: 0.0963 - val_accuracy: 0.9532\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1376 - accuracy: 0.9682 - val_loss: 0.0729 - val_accuracy: 0.9532\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 0.9827 - val_loss: 0.4924 - val_accuracy: 0.8480\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.9913 - val_loss: 0.1046 - val_accuracy: 0.9532\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9769 - val_loss: 0.1437 - val_accuracy: 0.9357\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9884 - val_loss: 0.0740 - val_accuracy: 0.9591\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.9769 - val_loss: 0.0951 - val_accuracy: 0.9532\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0964 - accuracy: 0.9798 - val_loss: 0.0879 - val_accuracy: 0.9532\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9624 - val_loss: 0.0782 - val_accuracy: 0.9591\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9827 - val_loss: 0.3211 - val_accuracy: 0.9064\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0445 - accuracy: 0.9827 - val_loss: 0.0742 - val_accuracy: 0.9532\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1310 - accuracy: 0.9682 - val_loss: 0.3917 - val_accuracy: 0.8830\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.0912 - accuracy: 0.9769 - val_loss: 0.0960 - val_accuracy: 0.9591\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9798 - val_loss: 0.1325 - val_accuracy: 0.9415\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.9740 - val_loss: 0.1308 - val_accuracy: 0.9415\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9827 - val_loss: 0.1331 - val_accuracy: 0.9591\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.9827 - val_loss: 0.2063 - val_accuracy: 0.9357\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.9740 - val_loss: 0.3584 - val_accuracy: 0.8947\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9942 - val_loss: 0.0781 - val_accuracy: 0.9532\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0609 - accuracy: 0.9855 - val_loss: 0.1735 - val_accuracy: 0.9357\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9711 - val_loss: 0.2805 - val_accuracy: 0.9240\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0459 - accuracy: 0.9769 - val_loss: 0.1352 - val_accuracy: 0.9474\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0479 - accuracy: 0.9855 - val_loss: 0.3826 - val_accuracy: 0.8889\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0566 - accuracy: 0.9798 - val_loss: 0.1301 - val_accuracy: 0.9415\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.9855 - val_loss: 0.1238 - val_accuracy: 0.9532\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.9884 - val_loss: 0.1170 - val_accuracy: 0.9474\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.9913 - val_loss: 0.0803 - val_accuracy: 0.9532\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9913 - val_loss: 0.0899 - val_accuracy: 0.9591\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9682 - val_loss: 0.2181 - val_accuracy: 0.9357\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.9884 - val_loss: 0.0882 - val_accuracy: 0.9591\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9855 - val_loss: 0.5236 - val_accuracy: 0.9064\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0496 - accuracy: 0.9769 - val_loss: 0.1258 - val_accuracy: 0.9474\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.3662 - accuracy: 0.9509 - val_loss: 0.1789 - val_accuracy: 0.9532\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9855 - val_loss: 0.1983 - val_accuracy: 0.9415\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9855 - val_loss: 0.1586 - val_accuracy: 0.9474\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0686 - accuracy: 0.9769 - val_loss: 0.3974 - val_accuracy: 0.9240\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0520 - accuracy: 0.9798 - val_loss: 0.1365 - val_accuracy: 0.9591\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9827 - val_loss: 0.1445 - val_accuracy: 0.9474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b1a45722e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, validation_split=0.33,epochs=100, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0999 - accuracy: 0.9594\n",
      "accuracy: 95.94%\n"
     ]
    }
   ],
   "source": [
    "scores=model.evaluate(x,y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset contains 36733 instances of 11 sensor measures aggregated over one hour (by means of average or sum) from a gas turbine. \n",
    "The Dataset includes gas turbine parameters (such as Turbine Inlet Temperature and Compressor Discharge pressure) in addition to the ambient variables.\n",
    "\n",
    "\n",
    "\n",
    "Problem statement: predicting turbine energy yield (TEY) using ambient variables as features.\n",
    "\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "The explanations of sensor measurements and their brief statistics are given below.\n",
    "\n",
    "Variable (Abbr.) Unit Min Max Mean\n",
    "Ambient temperature (AT) C â€“6.23 37.10 17.71\n",
    "Ambient pressure (AP) mbar 985.85 1036.56 1013.07\n",
    "Ambient humidity (AH) (%) 24.08 100.20 77.87\n",
    "Air filter difference pressure (AFDP) mbar 2.09 7.61 3.93\n",
    "Gas turbine exhaust pressure (GTEP) mbar 17.70 40.72 25.56\n",
    "Turbine inlet temperature (TIT) C 1000.85 1100.89 1081.43\n",
    "Turbine after temperature (TAT) C 511.04 550.61 546.16\n",
    "Compressor discharge pressure (CDP) mbar 9.85 15.16 12.06\n",
    "Turbine energy yield (TEY) MWH 100.02 179.50 133.51\n",
    "Carbon monoxide (CO) mg/m3 0.00 44.10 2.37\n",
    "Nitrogen oxides (NOx) mg/m3 25.90 119.91 65.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs=pd.read_csv(r'C:\\Users\\asus\\Desktop\\downlod\\gas_turbines.csv')\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n",
       "          82.722 ],\n",
       "       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n",
       "          82.776 ],\n",
       "       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n",
       "          82.468 ],\n",
       "       ...,\n",
       "       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n",
       "          90.912 ],\n",
       "       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n",
       "          93.227 ],\n",
       "       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n",
       "          92.498 ]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1=gs.values\n",
    "gs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n",
       "          82.722 ],\n",
       "       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n",
       "          82.776 ],\n",
       "       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n",
       "          82.468 ],\n",
       "       ...,\n",
       "       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n",
       "          90.912 ],\n",
       "       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n",
       "          93.227 ],\n",
       "       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n",
       "          92.498 ]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=gs1[:,[0,1,2,3,4,5,6,8,9,10]]\n",
    "Y=gs1[:,-4]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.25,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=MinMaxScaler()\n",
    "scaler.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35890393, 0.40602285, 0.91801706, ..., 0.34107329, 0.03084967,\n",
       "        0.48475958],\n",
       "       [0.55162803, 0.59086189, 0.72785444, ..., 0.42819611, 0.02833486,\n",
       "        0.43366477],\n",
       "       [0.69430373, 0.53478712, 0.55215014, ..., 0.14847583, 0.15186537,\n",
       "        0.33822331],\n",
       "       ...,\n",
       "       [0.29923532, 0.48494289, 0.94876603, ..., 0.77514199, 0.00101504,\n",
       "        0.41400706],\n",
       "       [0.64399376, 0.35825545, 0.50904718, ..., 0.04705791, 0.10100297,\n",
       "        0.36756316],\n",
       "       [0.3486443 , 0.24340602, 0.81637941, ..., 0.34416412, 0.00787964,\n",
       "        0.54170062]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "# add nodes for prediction\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "353/353 [==============================] - 2s 2ms/step - loss: 16163.3051\n",
      "Epoch 2/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 131.3408\n",
      "Epoch 3/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 27.0441\n",
      "Epoch 4/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 21.7133\n",
      "Epoch 5/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 18.5625\n",
      "Epoch 6/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 15.8137\n",
      "Epoch 7/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 13.4193\n",
      "Epoch 8/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 11.5536\n",
      "Epoch 9/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 9.6921\n",
      "Epoch 10/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 8.2868\n",
      "Epoch 11/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 6.9438\n",
      "Epoch 12/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 5.4822\n",
      "Epoch 13/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 4.5227\n",
      "Epoch 14/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 3.6362\n",
      "Epoch 15/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 2.8867\n",
      "Epoch 16/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 2.3255\n",
      "Epoch 17/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.9392\n",
      "Epoch 18/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.6081\n",
      "Epoch 19/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.3630\n",
      "Epoch 20/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.2308\n",
      "Epoch 21/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.1892\n",
      "Epoch 22/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.1211\n",
      "Epoch 23/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0961\n",
      "Epoch 24/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0587\n",
      "Epoch 25/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0702\n",
      "Epoch 26/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0752A: 0s - loss:\n",
      "Epoch 27/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0634\n",
      "Epoch 28/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0199\n",
      "Epoch 29/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0683\n",
      "Epoch 30/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0109\n",
      "Epoch 31/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0410\n",
      "Epoch 32/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0596\n",
      "Epoch 33/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0337\n",
      "Epoch 34/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0197\n",
      "Epoch 35/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 1.0185\n",
      "Epoch 36/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.1058\n",
      "Epoch 37/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 1.0199\n",
      "Epoch 38/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0327\n",
      "Epoch 39/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0638\n",
      "Epoch 40/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0525\n",
      "Epoch 41/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0801\n",
      "Epoch 42/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0543\n",
      "Epoch 43/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0381\n",
      "Epoch 44/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9872\n",
      "Epoch 45/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0339\n",
      "Epoch 46/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0719\n",
      "Epoch 47/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9959\n",
      "Epoch 48/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9845\n",
      "Epoch 49/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0528\n",
      "Epoch 50/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0140\n",
      "Epoch 51/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0263\n",
      "Epoch 52/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0231\n",
      "Epoch 53/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0483\n",
      "Epoch 54/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9594\n",
      "Epoch 55/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9954\n",
      "Epoch 56/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0214\n",
      "Epoch 57/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9863\n",
      "Epoch 58/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0168\n",
      "Epoch 59/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9485\n",
      "Epoch 60/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0367\n",
      "Epoch 61/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 1.0344\n",
      "Epoch 62/250\n",
      "353/353 [==============================] - 0s 1ms/step - loss: 0.9929\n",
      "Epoch 63/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0045\n",
      "Epoch 64/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0506\n",
      "Epoch 65/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0419\n",
      "Epoch 66/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9923\n",
      "Epoch 67/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0318\n",
      "Epoch 68/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9927\n",
      "Epoch 69/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9544\n",
      "Epoch 70/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0221\n",
      "Epoch 71/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0099\n",
      "Epoch 72/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9586\n",
      "Epoch 73/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9891\n",
      "Epoch 74/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0092\n",
      "Epoch 75/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9463\n",
      "Epoch 76/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9992\n",
      "Epoch 77/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0279\n",
      "Epoch 78/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9665\n",
      "Epoch 79/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9836\n",
      "Epoch 80/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9885\n",
      "Epoch 81/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9827\n",
      "Epoch 82/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9825\n",
      "Epoch 83/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9590\n",
      "Epoch 84/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9736\n",
      "Epoch 85/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0262\n",
      "Epoch 86/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0057\n",
      "Epoch 87/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9471\n",
      "Epoch 88/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 1.0319A: 0s - los\n",
      "Epoch 89/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9696\n",
      "Epoch 90/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0247\n",
      "Epoch 91/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0188\n",
      "Epoch 92/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9664\n",
      "Epoch 93/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9719\n",
      "Epoch 94/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9846\n",
      "Epoch 95/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9600\n",
      "Epoch 96/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9623\n",
      "Epoch 97/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9978\n",
      "Epoch 98/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9634\n",
      "Epoch 99/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9559\n",
      "Epoch 100/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9833\n",
      "Epoch 101/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9333\n",
      "Epoch 102/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9838\n",
      "Epoch 103/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9408A: 0s - loss: 0\n",
      "Epoch 104/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9908\n",
      "Epoch 105/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0281\n",
      "Epoch 106/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9875\n",
      "Epoch 107/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9598\n",
      "Epoch 108/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9604\n",
      "Epoch 109/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9943\n",
      "Epoch 110/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9428\n",
      "Epoch 111/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0121\n",
      "Epoch 112/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9094\n",
      "Epoch 113/250\n",
      "353/353 [==============================] - 0s 1ms/step - loss: 0.9882\n",
      "Epoch 114/250\n",
      "353/353 [==============================] - 0s 1ms/step - loss: 0.9469\n",
      "Epoch 115/250\n",
      "353/353 [==============================] - 0s 1ms/step - loss: 0.9958\n",
      "Epoch 116/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9311\n",
      "Epoch 117/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9160\n",
      "Epoch 118/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 1.0010\n",
      "Epoch 119/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9633\n",
      "Epoch 120/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9009\n",
      "Epoch 121/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9753\n",
      "Epoch 122/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9773\n",
      "Epoch 123/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9247\n",
      "Epoch 124/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9535\n",
      "Epoch 125/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9201\n",
      "Epoch 126/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9349\n",
      "Epoch 127/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9527\n",
      "Epoch 128/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9726\n",
      "Epoch 129/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9608\n",
      "Epoch 130/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9751\n",
      "Epoch 131/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9740\n",
      "Epoch 132/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9191\n",
      "Epoch 133/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9294\n",
      "Epoch 134/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9102\n",
      "Epoch 135/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9718\n",
      "Epoch 136/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9362\n",
      "Epoch 137/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9496\n",
      "Epoch 138/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9446\n",
      "Epoch 139/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9732\n",
      "Epoch 140/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9559\n",
      "Epoch 141/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9295\n",
      "Epoch 142/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9303\n",
      "Epoch 143/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9301\n",
      "Epoch 144/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9985\n",
      "Epoch 145/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9342\n",
      "Epoch 146/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9638\n",
      "Epoch 147/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8799\n",
      "Epoch 148/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9462\n",
      "Epoch 149/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9441\n",
      "Epoch 150/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9317A: 0s - loss: \n",
      "Epoch 151/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8935\n",
      "Epoch 152/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9286\n",
      "Epoch 153/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9199\n",
      "Epoch 154/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9247\n",
      "Epoch 155/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9220\n",
      "Epoch 156/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9236\n",
      "Epoch 157/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9436\n",
      "Epoch 158/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9702\n",
      "Epoch 159/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9474\n",
      "Epoch 160/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9193\n",
      "Epoch 161/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8957\n",
      "Epoch 162/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9449\n",
      "Epoch 163/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9333\n",
      "Epoch 164/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9533\n",
      "Epoch 165/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9069\n",
      "Epoch 166/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9405\n",
      "Epoch 167/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9466\n",
      "Epoch 168/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9443\n",
      "Epoch 169/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8861\n",
      "Epoch 170/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9034\n",
      "Epoch 171/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9261\n",
      "Epoch 172/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9141\n",
      "Epoch 173/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9368\n",
      "Epoch 174/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8811\n",
      "Epoch 175/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9430\n",
      "Epoch 176/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9183\n",
      "Epoch 177/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9249\n",
      "Epoch 178/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8708\n",
      "Epoch 179/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8894\n",
      "Epoch 180/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9252\n",
      "Epoch 181/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9614\n",
      "Epoch 182/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9017\n",
      "Epoch 183/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9335\n",
      "Epoch 184/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9183\n",
      "Epoch 185/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9076\n",
      "Epoch 186/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9081\n",
      "Epoch 187/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9052\n",
      "Epoch 188/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9529\n",
      "Epoch 189/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9250\n",
      "Epoch 190/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8939\n",
      "Epoch 191/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9159\n",
      "Epoch 192/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9095\n",
      "Epoch 193/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9126\n",
      "Epoch 194/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8883\n",
      "Epoch 195/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9073\n",
      "Epoch 196/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9325\n",
      "Epoch 197/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8915\n",
      "Epoch 198/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8673\n",
      "Epoch 199/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8827\n",
      "Epoch 200/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9190\n",
      "Epoch 201/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9233\n",
      "Epoch 202/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9075\n",
      "Epoch 203/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8844\n",
      "Epoch 204/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8923\n",
      "Epoch 205/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9046\n",
      "Epoch 206/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8931\n",
      "Epoch 207/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9206\n",
      "Epoch 208/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8655\n",
      "Epoch 209/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9288\n",
      "Epoch 210/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9117\n",
      "Epoch 211/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9481\n",
      "Epoch 212/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8788\n",
      "Epoch 213/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9118\n",
      "Epoch 214/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9331\n",
      "Epoch 215/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8721\n",
      "Epoch 216/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8670\n",
      "Epoch 217/250\n",
      "353/353 [==============================] - 0s 1ms/step - loss: 0.8904\n",
      "Epoch 218/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9114\n",
      "Epoch 219/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8907\n",
      "Epoch 220/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8799A: 0s - lo\n",
      "Epoch 221/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8697\n",
      "Epoch 222/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8718\n",
      "Epoch 223/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9137\n",
      "Epoch 224/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8806\n",
      "Epoch 225/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8704\n",
      "Epoch 226/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9214\n",
      "Epoch 227/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8748\n",
      "Epoch 228/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9331\n",
      "Epoch 229/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9014\n",
      "Epoch 230/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8857\n",
      "Epoch 231/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8626\n",
      "Epoch 232/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9116\n",
      "Epoch 233/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8963\n",
      "Epoch 234/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8768\n",
      "Epoch 235/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8309\n",
      "Epoch 236/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9224\n",
      "Epoch 237/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8876\n",
      "Epoch 238/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8677\n",
      "Epoch 239/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8925\n",
      "Epoch 240/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9153\n",
      "Epoch 241/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8723\n",
      "Epoch 242/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9103\n",
      "Epoch 243/250\n",
      "353/353 [==============================] - 1s 1ms/step - loss: 0.9013\n",
      "Epoch 244/250\n",
      "353/353 [==============================] - 0s 1ms/step - loss: 0.8955\n",
      "Epoch 245/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8749\n",
      "Epoch 246/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9200\n",
      "Epoch 247/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8619\n",
      "Epoch 248/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.9046\n",
      "Epoch 249/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8784\n",
      "Epoch 250/250\n",
      "353/353 [==============================] - 1s 2ms/step - loss: 0.8767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b1af75ac10>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as plot\n",
    "model_loss = pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[132.89326],\n",
       "       [134.27205],\n",
       "       [111.37721],\n",
       "       ...,\n",
       "       [160.92926],\n",
       "       [104.80077],\n",
       "       [133.24117]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7105437517166138"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score = model.evaluate(x_test,y_test,verbose=0)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6322248241343397"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7105435025622328"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1b1b0f45fd0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
